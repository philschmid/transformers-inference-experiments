{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e9a818-7c57-466d-b4c8-158533f615b5",
   "metadata": {},
   "source": [
    "# Performance Tuning\n",
    "\n",
    "This Notebook includes examples on how you can tune performance with the `aws-neuron-sdk` for you ðŸ¤— Transformer models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06d8372-b69a-469f-9464-1aa1d8ab64f4",
   "metadata": {},
   "source": [
    "## Batching\n",
    "\n",
    "**batching** it is achieved by loading the data into an on-chip cache and reusing it multiple times for multiple different model-inputs.  \n",
    "=> batching is preferred for applications that aim to optimize throughput and cost at the expense of latency.  \n",
    "\n",
    "---\n",
    "\n",
    "To enable the batching optimization, we first need to compile the model for a target `batch-size`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf9db545-2943-448f-881e-989da69113fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54c9d19c07e4831a9f42d94992f8f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=747.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec64c699bd5457eb7c7d6694beec047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=898822.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec7df67c817451d944a466698f41121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=456318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd23ca46134a443cbb0b17b0983ceac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=150.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8668a35ba14fd089b7b8131ffebc8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=498679497.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow  # to workaround a protobuf version conflict issue\n",
    "import torch\n",
    "import torch.neuron\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import transformers\n",
    "  \n",
    "# Build tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\", return_dict=False)\n",
    "# Setup some example inputs\n",
    "positive_sequence = \"This is a nice sentence about very kind guy from the east.\"\n",
    "negative_sequence = \"You fucking bastard.\"\n",
    "\n",
    "max_length=128\n",
    "batch_size=6\n",
    "\n",
    "paraphrase = tokenizer(positive_sequence, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "example_inputs_paraphrase = (\n",
    "    torch.cat([paraphrase['input_ids']] * batch_size,0), \n",
    "    torch.cat([paraphrase['attention_mask']] * batch_size,0)\n",
    ")\n",
    "# test model\n",
    "outputs = model(**paraphrase)\n",
    "assert 2 == outputs[0][0].argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c9d30-662b-4574-a65c-2700027e37ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyze the model - this will show operator support and operator count\n",
    "torch.neuron.analyze_model(model, example_inputs=example_inputs_paraphrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a5df626-fa54-4018-8285-84adff30f717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/modeling_utils.py:1790: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors\n",
      "INFO:Neuron:There are 5 ops of 3 different types in the TorchScript that are not compiled by neuron-cc: aten::embedding, aten::cumsum, aten::type_as, (For more information see https://github.com/aws/aws-neuron-sdk/blob/master/release-notes/neuron-cc-ops/neuron-cc-ops-pytorch.md)\n",
      "INFO:Neuron:Number of arithmetic operators (pre-compilation) before = 722, fused = 696, percent fused = 96.4%\n",
      "INFO:Neuron:compiling function _NeuronGraph$661 with neuron-cc\n",
      "INFO:Neuron:Compiling with command line: '/opt/conda/bin/neuron-cc compile /tmp/tmp33ogt4ii/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /tmp/tmp33ogt4ii/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[6, 128, 768], \"float32\"], \"1:0\": [[6, 1, 1, 128], \"float32\"]}, \"outputs\": [\"Add_133:0\"]} --verbose 35'\n",
      "/opt/conda/lib/python3.7/site-packages/torch/jit/_trace.py:441: UserWarning: Neuron runtime cannot be initialized; falling back to CPU execution\n",
      "Tensor output are ** NOT CALCULATED ** during CPU execution and only indicate tensor shape (Triggered internally at  /opt/workspace/KaenaPyTorchRuntime/neuron_op/neuron_op_impl.cpp:38.)\n",
      "  outs = wrap_retval(mod(*_clone_inputs(inputs)))\n",
      "/opt/conda/lib/python3.7/site-packages/torch_neuron/graph.py:383: UserWarning: Neuron runtime cannot be initialized; falling back to CPU execution\n",
      "Tensor output are ** NOT CALCULATED ** during CPU execution and only indicate tensor shape (Triggered internally at  /opt/workspace/KaenaPyTorchRuntime/neuron_op/neuron_op_impl.cpp:38.)\n",
      "  return self.func(*inputs)\n",
      "INFO:Neuron:Number of arithmetic operators (post-compilation) before = 722, compiled = 696, percent compiled = 96.4%\n",
      "INFO:Neuron:The neuron partitioner created 1 sub-graphs\n",
      "INFO:Neuron:Neuron successfully compiled 1 sub-graphs, Total fused subgraphs = 1, Percent of model sub-graphs successfully compiled = 100.0%\n",
      "INFO:Neuron:Compiled these operators (and operator counts) to Neuron:\n",
      "INFO:Neuron: => aten::Int: 96\n",
      "INFO:Neuron: => aten::add: 108\n",
      "INFO:Neuron: => aten::addmm: 2\n",
      "INFO:Neuron: => aten::contiguous: 12\n",
      "INFO:Neuron: => aten::div: 12\n",
      "INFO:Neuron: => aten::dropout: 39\n",
      "INFO:Neuron: => aten::gelu: 12\n",
      "INFO:Neuron: => aten::layer_norm: 25\n",
      "INFO:Neuron: => aten::matmul: 96\n",
      "INFO:Neuron: => aten::permute: 48\n",
      "INFO:Neuron: => aten::select: 1\n",
      "INFO:Neuron: => aten::size: 96\n",
      "INFO:Neuron: => aten::slice: 2\n",
      "INFO:Neuron: => aten::softmax: 12\n",
      "INFO:Neuron: => aten::t: 74\n",
      "INFO:Neuron: => aten::tanh: 1\n",
      "INFO:Neuron: => aten::transpose: 12\n",
      "INFO:Neuron: => aten::view: 48\n",
      "INFO:Neuron:Not compiled operators (and operator counts) to Neuron:\n",
      "INFO:Neuron: => aten::Int: 2 [supported]\n",
      "INFO:Neuron: => aten::add: 4 [supported]\n",
      "INFO:Neuron: => aten::cumsum: 1 [not supported]\n",
      "INFO:Neuron: => aten::embedding: 3 [not supported]\n",
      "INFO:Neuron: => aten::mul: 2 [supported]\n",
      "INFO:Neuron: => aten::ne: 1 [supported]\n",
      "INFO:Neuron: => aten::rsub: 1 [supported]\n",
      "INFO:Neuron: => aten::size: 2 [supported]\n",
      "INFO:Neuron: => aten::slice: 2 [supported]\n",
      "INFO:Neuron: => aten::to: 4 [supported]\n",
      "INFO:Neuron: => aten::type_as: 1 [not supported]\n",
      "INFO:Neuron: => aten::unsqueeze: 2 [supported]\n",
      "INFO:Neuron: => aten::zeros: 1 [supported]\n"
     ]
    }
   ],
   "source": [
    "# Run torch.neuron.trace to generate a TorchScript that is optimized by AWS Neuron\n",
    "model_neuron_batch = torch.neuron.trace(model, example_inputs_paraphrase)\n",
    "\n",
    "outputs = model_neuron_batch(*example_inputs_paraphrase)\n",
    "\n",
    "for output in outputs[0]:\n",
    "    assert 2 == output.argmax().item()\n",
    "    \n",
    "# Save the batched model\n",
    "model_neuron_batch.save('roberta_neuron_b{}.pt'.format(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491e76c0-129f-49c9-bb48-8f8ef660bbc2",
   "metadata": {},
   "source": [
    "## Test `neuron_model` vs vanilla model\n",
    "\n",
    "test will be run with `batch_size=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c02d267a-2947-4beb-9d3d-eb4833e856f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset tweet_eval (/root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/79e21f7659e902ea14f624232219492d972fe5e0f9d8c94363acc7f916a6be48)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082e674ef2244ab69f110d31c2a15244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=12284.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_dataset = load_dataset('tweet_eval', 'sentiment', split='test')\n",
    "\n",
    "processed_dataset = raw_dataset.map(lambda seq: tokenizer(seq['text'], max_length=max_length, padding=True, truncation=True, return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ecc647ae-a333-4e61-ad5c-087b5a9440ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py:727: UserWarning: Neuron runtime cannot be initialized; falling back to CPU execution\n",
      "Tensor output are ** NOT CALCULATED ** during CPU execution and only indicate tensor shape (Triggered internally at  /opt/workspace/KaenaPyTorchRuntime/neuron_op/neuron_op_impl.cpp:38.)\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>total_time</th>\n",
       "      <th>average_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>0.705</td>\n",
       "      <td>170575.3ms</td>\n",
       "      <td>170.5753ms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neuron</td>\n",
       "      <td>0.197</td>\n",
       "      <td>1817.6000000000001ms</td>\n",
       "      <td>1.8176ms</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_type  accuracy            total_time average_time\n",
       "0    pytorch     0.705            170575.3ms   170.5753ms\n",
       "1     neuron     0.197  1817.6000000000001ms     1.8176ms"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def do_test(processed_dataset,model,model_type):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    processed_dataset = processed_dataset.select(range(1000))\n",
    "    model_start = time.perf_counter()\n",
    "    model.eval()\n",
    "#     model_type = 'neuron' if isinstance(model, torch.jit.ScriptModule) else 'torch'\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(processed_dataset):\n",
    "            input_ids = torch.tensor(batch['input_ids'])\n",
    "            attention_mask = torch.tensor(batch['attention_mask'])\n",
    "            outputs = model(*[input_ids,attention_mask])\n",
    "            predictions = outputs[0][0].argmax().item()\n",
    "            metric.add_batch(predictions=[predictions],references=[batch[\"label\"]])\n",
    "        \n",
    "    eval_metric = metric.compute()\n",
    "    model_stop = time.perf_counter()\n",
    "    total_time = round(model_stop - model_start,4)*1000\n",
    "    average_time =  round(total_time/len(processed_dataset),4)\n",
    "    return {'model_type':model_type,**eval_metric,'total_time':f\"{total_time}ms\",'average_time':f\"{average_time}ms\"}   \n",
    "\n",
    "\n",
    "model_res=do_test(processed_dataset, model,'pytorch')\n",
    "model_neuron_res = do_test(processed_dataset, model_neuron_batch,'neuron')\n",
    "\n",
    "\n",
    "df = pd.DataFrame([model_res,model_neuron_res])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c93bcc-854a-494b-8887-839a5d3294a9",
   "metadata": {},
   "source": [
    "## batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fe93af23-cf86-40f8-aba3-7fc100c9bac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/79e21f7659e902ea14f624232219492d972fe5e0f9d8c94363acc7f916a6be48/cache-9dce7006cabdbad9.arrow\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference of 12284 examples with batch_size 1 took 20.0836 seconds\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "import time\n",
    "\n",
    "max_length = 128\n",
    "batch_size = 6\n",
    "\n",
    "\n",
    "batch_raw_dataset = raw_dataset.map(lambda seq: tokenizer(seq['text'],padding=\"max_length\", max_length=max_length,truncation=True))\n",
    "batch_raw_dataset = batch_raw_dataset.remove_columns('text')\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(batch_raw_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                        collate_fn=DataCollatorWithPadding(tokenizer)\n",
    "                                         )\n",
    "\n",
    "model_start = time.perf_counter()\n",
    "model_neuron_batch.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "            input_ids = torch.tensor(batch['input_ids'])\n",
    "            attention_mask = torch.tensor(batch['attention_mask'])\n",
    "            outputs = model_neuron_batch(*[input_ids,attention_mask])\n",
    "            predictions = outputs[0][0].argmax().item()\n",
    "\n",
    "model_stop = time.perf_counter()\n",
    "total_time = round(model_stop - model_start,4)\n",
    "\n",
    "\n",
    "print(f\"inference of {len(batch_raw_dataset)} examples with batch_size {batch_size} took {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0281a66d-4672-44c4-9d38-37bfa76f5ce2",
   "metadata": {},
   "source": [
    "tried different batches\n",
    "\n",
    "    inference of 12284 examples with batch_size 1 took 20.0836 seconds\n",
    "    inference of 12284 examples with batch_size 3 took 9.1281 seconds\n",
    "    inference of 12284 examples with batch_size 6 took 6.8953 seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039d29c2-5282-4a52-819f-623e29c0de4d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3438ae20-f580-4634-990f-c6cf562e26f4",
   "metadata": {},
   "source": [
    "# Mixed Precission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7257a058-0c93-4145-9499-c172af7d68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow  # to workaround a protobuf version conflict issue\n",
    "import torch\n",
    "import torch.neuron\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import transformers\n",
    "  \n",
    "# Build tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\", return_dict=False)\n",
    "# Setup some example inputs\n",
    "positive_sequence = \"This is a nice sentence about very kind guy from the east.\"\n",
    "negative_sequence = \"You fucking bastard.\"\n",
    "\n",
    "max_length=128\n",
    "batch_size=6\n",
    "\n",
    "paraphrase = tokenizer(positive_sequence, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "example_inputs_paraphrase = paraphrase['input_ids'], paraphrase['attention_mask']\n",
    "\n",
    "# test model\n",
    "outputs = model(**paraphrase)\n",
    "assert 2 == outputs[0][0].argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a5cee599-b54d-4ce0-830d-e81a69aafbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/modeling_utils.py:1790: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors\n",
      "INFO:Neuron:There are 5 ops of 3 different types in the TorchScript that are not compiled by neuron-cc: aten::embedding, aten::cumsum, aten::type_as, (For more information see https://github.com/aws/aws-neuron-sdk/blob/master/release-notes/neuron-cc-ops/neuron-cc-ops-pytorch.md)\n",
      "INFO:Neuron:Number of arithmetic operators (pre-compilation) before = 722, fused = 696, percent fused = 96.4%\n",
      "INFO:Neuron:Compiler args type is <class 'list'> value is ['--fp32-cast=matmult']\n",
      "INFO:Neuron:compiling function _NeuronGraph$1987 with neuron-cc\n",
      "INFO:Neuron:Compiling with command line: '/opt/conda/bin/neuron-cc compile /tmp/tmpfxaew0b1/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /tmp/tmpfxaew0b1/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 128, 768], \"float32\"], \"1:0\": [[1, 1, 1, 128], \"float32\"]}, \"outputs\": [\"Add_133:0\"]} --fp32-cast=matmult --verbose 35'\n",
      "/opt/conda/lib/python3.7/site-packages/torch/jit/_trace.py:441: UserWarning: Neuron runtime cannot be initialized; falling back to CPU execution\n",
      "Tensor output are ** NOT CALCULATED ** during CPU execution and only indicate tensor shape (Triggered internally at  /opt/workspace/KaenaPyTorchRuntime/neuron_op/neuron_op_impl.cpp:38.)\n",
      "  outs = wrap_retval(mod(*_clone_inputs(inputs)))\n",
      "/opt/conda/lib/python3.7/site-packages/torch_neuron/graph.py:383: UserWarning: Neuron runtime cannot be initialized; falling back to CPU execution\n",
      "Tensor output are ** NOT CALCULATED ** during CPU execution and only indicate tensor shape (Triggered internally at  /opt/workspace/KaenaPyTorchRuntime/neuron_op/neuron_op_impl.cpp:38.)\n",
      "  return self.func(*inputs)\n",
      "INFO:Neuron:Number of arithmetic operators (post-compilation) before = 722, compiled = 696, percent compiled = 96.4%\n",
      "INFO:Neuron:The neuron partitioner created 1 sub-graphs\n",
      "INFO:Neuron:Neuron successfully compiled 1 sub-graphs, Total fused subgraphs = 1, Percent of model sub-graphs successfully compiled = 100.0%\n",
      "INFO:Neuron:Compiled these operators (and operator counts) to Neuron:\n",
      "INFO:Neuron: => aten::Int: 96\n",
      "INFO:Neuron: => aten::add: 108\n",
      "INFO:Neuron: => aten::addmm: 2\n",
      "INFO:Neuron: => aten::contiguous: 12\n",
      "INFO:Neuron: => aten::div: 12\n",
      "INFO:Neuron: => aten::dropout: 39\n",
      "INFO:Neuron: => aten::gelu: 12\n",
      "INFO:Neuron: => aten::layer_norm: 25\n",
      "INFO:Neuron: => aten::matmul: 96\n",
      "INFO:Neuron: => aten::permute: 48\n",
      "INFO:Neuron: => aten::select: 1\n",
      "INFO:Neuron: => aten::size: 96\n",
      "INFO:Neuron: => aten::slice: 2\n",
      "INFO:Neuron: => aten::softmax: 12\n",
      "INFO:Neuron: => aten::t: 74\n",
      "INFO:Neuron: => aten::tanh: 1\n",
      "INFO:Neuron: => aten::transpose: 12\n",
      "INFO:Neuron: => aten::view: 48\n",
      "INFO:Neuron:Not compiled operators (and operator counts) to Neuron:\n",
      "INFO:Neuron: => aten::Int: 2 [supported]\n",
      "INFO:Neuron: => aten::add: 4 [supported]\n",
      "INFO:Neuron: => aten::cumsum: 1 [not supported]\n",
      "INFO:Neuron: => aten::embedding: 3 [not supported]\n",
      "INFO:Neuron: => aten::mul: 2 [supported]\n",
      "INFO:Neuron: => aten::ne: 1 [supported]\n",
      "INFO:Neuron: => aten::rsub: 1 [supported]\n",
      "INFO:Neuron: => aten::size: 2 [supported]\n",
      "INFO:Neuron: => aten::slice: 2 [supported]\n",
      "INFO:Neuron: => aten::to: 4 [supported]\n",
      "INFO:Neuron: => aten::type_as: 1 [not supported]\n",
      "INFO:Neuron: => aten::unsqueeze: 2 [supported]\n",
      "INFO:Neuron: => aten::zeros: 1 [supported]\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py:727: UserWarning: Neuron runtime cannot be initialized; falling back to CPU execution\n",
      "Tensor output are ** NOT CALCULATED ** during CPU execution and only indicate tensor shape (Triggered internally at  /opt/workspace/KaenaPyTorchRuntime/neuron_op/neuron_op_impl.cpp:38.)\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "compiler_args = ['--fp32-cast=matmult']\n",
    "\n",
    "# Run torch.neuron.trace to generate a TorchScript that is optimized by AWS Neuron\n",
    "model_neuron_mixed = torch.neuron.trace(model, \n",
    "                                        example_inputs=example_inputs_paraphrase,\n",
    "                                        compiler_args=compiler_args)\n",
    "\n",
    "outputs = model_neuron_mixed(*example_inputs_paraphrase)\n",
    "\n",
    "assert 2 == output.argmax().item()\n",
    "    \n",
    "# Save the batched model\n",
    "model_neuron_mixed.save('roberta_neuron_mixed.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26163b18-71c3-451d-9278-98318b43fc04",
   "metadata": {},
   "source": [
    "## test again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "56532f02-761b-4d21-b37e-e09cfa38d588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py:727: UserWarning: Neuron runtime cannot be initialized; falling back to CPU execution\n",
      "Tensor output are ** NOT CALCULATED ** during CPU execution and only indicate tensor shape (Triggered internally at  /opt/workspace/KaenaPyTorchRuntime/neuron_op/neuron_op_impl.cpp:38.)\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>total_time</th>\n",
       "      <th>average_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>0.705</td>\n",
       "      <td>70190.29999999999ms</td>\n",
       "      <td>70.1903ms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neuron</td>\n",
       "      <td>0.197</td>\n",
       "      <td>995.8000000000001ms</td>\n",
       "      <td>0.9958ms</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_type  accuracy           total_time average_time\n",
       "0    pytorch     0.705  70190.29999999999ms    70.1903ms\n",
       "1     neuron     0.197  995.8000000000001ms     0.9958ms"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_res=do_test(processed_dataset, model,'pytorch')\n",
    "model_neuron_res = do_test(processed_dataset, model_neuron_mixed,'neuron')\n",
    "\n",
    "\n",
    "df = pd.DataFrame([model_res,model_neuron_res])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4a66c357-26f1-4291-924b-84d7340a095d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py:727: UserWarning: Neuron runtime cannot be initialized; falling back to CPU execution\n",
      "Tensor output are ** NOT CALCULATED ** during CPU execution and only indicate tensor shape (Triggered internally at  /opt/workspace/KaenaPyTorchRuntime/neuron_op/neuron_op_impl.cpp:38.)\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>total_time</th>\n",
       "      <th>average_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>0.705</td>\n",
       "      <td>72931.2ms</td>\n",
       "      <td>72.9312ms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neuron</td>\n",
       "      <td>0.197</td>\n",
       "      <td>900.6999999999999ms</td>\n",
       "      <td>0.9007ms</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_type  accuracy           total_time average_time\n",
       "0    pytorch     0.705            72931.2ms    72.9312ms\n",
       "1     neuron     0.197  900.6999999999999ms     0.9007ms"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_res=do_test(processed_dataset, model,'pytorch')\n",
    "model_neuron_res = do_test(processed_dataset, model_neuron_mixed,'neuron')\n",
    "\n",
    "\n",
    "df = pd.DataFrame([model_res,model_neuron_res])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
