{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling and Deploying HuggingFace Pretrained BERT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this tutorial we will compile and deploy BERT-base version of HuggingFace BERT for Inferentia. The full list of HuggingFace's pretrained BERT models can be found in the BERT section on this page https://huggingface.co/transformers/pretrained_models.html. \n",
    "\n",
    "This Jupyter notebook should be run on an instance which is inf1.6xlarge or larger. The compile part of this tutorial requires inf1.6xlarge and not the inference itself. For simplicity we will run this tutorial on inf1.6xlarge but in real life scenario the compilation should be done on a compute instance and the deployment on inf1 instance to save costs.\n",
    "\n",
    "Before running the following verify this Jupyter notebook is running “conda_aws_neuron_pytorch_p36” kernel. You can select the Kernel from the “Kernel -> Change Kernel” option on the top of this Jupyter notebook page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model into an AWS Neuron optimized TorchScript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5862e71e1240fe97a68df48925fe3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=433297515.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py:195: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/modeling_utils.py:1790: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors\n",
      "INFO:Neuron:There are 3 ops of 1 different types in the TorchScript that are not compiled by neuron-cc: aten::embedding, (For more information see https://github.com/aws/aws-neuron-sdk/blob/master/release-notes/neuron-cc-ops/neuron-cc-ops-pytorch.md)\n",
      "INFO:Neuron:Number of arithmetic operators (pre-compilation) before = 711, fused = 694, percent fused = 97.61%\n",
      "INFO:Neuron:compiling function _NeuronGraph$661 with neuron-cc\n",
      "INFO:Neuron:Compiling with command line: '/opt/conda/bin/neuron-cc compile /tmp/tmpepbk8pfy/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /tmp/tmpepbk8pfy/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 128, 768], \"float32\"], \"1:0\": [[1, 1, 1, 128], \"float32\"]}, \"outputs\": [\"Add_133:0\"]} --verbose 35'\n",
      "/opt/conda/lib/python3.7/site-packages/torch/jit/_trace.py:441: UserWarning: Neuron runtime cannot be initialized; falling back to CPU execution\n",
      "Tensor output are ** NOT CALCULATED ** during CPU execution and only indicate tensor shape (Triggered internally at  /opt/workspace/KaenaPyTorchRuntime/neuron_op/neuron_op_impl.cpp:38.)\n",
      "  outs = wrap_retval(mod(*_clone_inputs(inputs)))\n",
      "/opt/conda/lib/python3.7/site-packages/torch_neuron/graph.py:383: UserWarning: Neuron runtime cannot be initialized; falling back to CPU execution\n",
      "Tensor output are ** NOT CALCULATED ** during CPU execution and only indicate tensor shape (Triggered internally at  /opt/workspace/KaenaPyTorchRuntime/neuron_op/neuron_op_impl.cpp:38.)\n",
      "  return self.func(*inputs)\n",
      "INFO:Neuron:Number of arithmetic operators (post-compilation) before = 711, compiled = 694, percent compiled = 97.61%\n",
      "INFO:Neuron:The neuron partitioner created 1 sub-graphs\n",
      "INFO:Neuron:Neuron successfully compiled 1 sub-graphs, Total fused subgraphs = 1, Percent of model sub-graphs successfully compiled = 100.0%\n",
      "INFO:Neuron:Compiled these operators (and operator counts) to Neuron:\n",
      "INFO:Neuron: => aten::Int: 96\n",
      "INFO:Neuron: => aten::add: 108\n",
      "INFO:Neuron: => aten::addmm: 2\n",
      "INFO:Neuron: => aten::contiguous: 12\n",
      "INFO:Neuron: => aten::div: 12\n",
      "INFO:Neuron: => aten::dropout: 38\n",
      "INFO:Neuron: => aten::gelu: 12\n",
      "INFO:Neuron: => aten::layer_norm: 25\n",
      "INFO:Neuron: => aten::matmul: 96\n",
      "INFO:Neuron: => aten::permute: 48\n",
      "INFO:Neuron: => aten::select: 1\n",
      "INFO:Neuron: => aten::size: 96\n",
      "INFO:Neuron: => aten::slice: 1\n",
      "INFO:Neuron: => aten::softmax: 12\n",
      "INFO:Neuron: => aten::t: 74\n",
      "INFO:Neuron: => aten::tanh: 1\n",
      "INFO:Neuron: => aten::transpose: 12\n",
      "INFO:Neuron: => aten::view: 48\n",
      "INFO:Neuron:Not compiled operators (and operator counts) to Neuron:\n",
      "INFO:Neuron: => aten::Int: 1 [supported]\n",
      "INFO:Neuron: => aten::add: 3 [supported]\n",
      "INFO:Neuron: => aten::embedding: 3 [not supported]\n",
      "INFO:Neuron: => aten::mul: 1 [supported]\n",
      "INFO:Neuron: => aten::rsub: 1 [supported]\n",
      "INFO:Neuron: => aten::size: 1 [supported]\n",
      "INFO:Neuron: => aten::slice: 4 [supported]\n",
      "INFO:Neuron: => aten::to: 1 [supported]\n",
      "INFO:Neuron: => aten::unsqueeze: 2 [supported]\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py:727: UserWarning: Neuron runtime cannot be initialized; falling back to CPU execution\n",
      "Tensor output are ** NOT CALCULATED ** during CPU execution and only indicate tensor shape (Triggered internally at  /opt/workspace/KaenaPyTorchRuntime/neuron_op/neuron_op_impl.cpp:38.)\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow  # to workaround a protobuf version conflict issue\n",
    "import torch\n",
    "import torch.neuron\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import transformers\n",
    "\n",
    "# Build tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\", return_dict=False)\n",
    "\n",
    "# Setup some example inputs\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "max_length=128\n",
    "paraphrase = tokenizer.encode_plus(sequence_0, sequence_2, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Run the original PyTorch model on compilation exaple\n",
    "paraphrase_classification_logits = model(**paraphrase)[0]\n",
    "\n",
    "# Convert example inputs to a format that is compatible with TorchScript tracing\n",
    "example_inputs_paraphrase = paraphrase['input_ids'], paraphrase['attention_mask'], paraphrase['token_type_ids']\n",
    "example_inputs_not_paraphrase = not_paraphrase['input_ids'], not_paraphrase['attention_mask'], not_paraphrase['token_type_ids']\n",
    "\n",
    "# Run torch.neuron.trace to generate a TorchScript that is optimized by AWS Neuron\n",
    "model_neuron = torch.neuron.trace(model, example_inputs_paraphrase)\n",
    "\n",
    "# Verify the TorchScript works on both example inputs\n",
    "paraphrase_classification_logits_neuron = model_neuron(*example_inputs_paraphrase)\n",
    "not_paraphrase_classification_logits_neuron = model_neuron(*example_inputs_not_paraphrase)\n",
    "\n",
    "# Save the TorchScript for later use\n",
    "model_neuron.save('bert_neuron.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may inspect `model_neuron.graph` to see which part is running on CPU versus running on the accelerator. All native `aten` operators in the graph will be running on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_neuron.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Single example TorchScript with default vanilla Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "TorchScript Neuron inference took 5.8ms and class was paraphrase\n",
      "PyTorch inference took 150.5ms and class was not paraphrase\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "sequence = [\"The company HuggingFace is based in New York City\",\n",
    "           # \"Apples are especially bad for your health\"\n",
    "           \"HuggingFace's headquarters are situated in Manhattan\"]\n",
    "\n",
    "model_neuron = torch.jit.load('bert_neuron.pt')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\", return_dict=False)\n",
    "\n",
    "\n",
    "classes = ['not paraphrase', 'paraphrase']\n",
    "\n",
    "def tokenize(sequence,max_length=128):\n",
    "    tokenized_seq = tokenizer.encode_plus(sequence, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    return tokenized_seq['input_ids'], tokenized_seq['attention_mask'], tokenized_seq['token_type_ids']\n",
    "\n",
    "    \n",
    "def run_benchmark(model,inputs,device=None):\n",
    "    with torch.no_grad():\n",
    "        if isinstance(model, torch.jit.ScriptModule):\n",
    "            model_start = time.perf_counter()\n",
    "            outputs = model(*inputs)\n",
    "            model_stop = time.perf_counter()\n",
    "            pred = classes[outputs[0][0].argmax().item()]\n",
    "            print(f\"TorchScript Neuron inference took {round(model_stop - model_start,4) * 1000}ms and class was {pred}\")\n",
    "        else:\n",
    "            model_start = time.perf_counter()\n",
    "            outputs = model(*inputs)\n",
    "            model_stop = time.perf_counter()\n",
    "            pred = classes[outputs[0][0].argmax().item()]\n",
    "            print(f\"PyTorch inference took {round(model_stop - model_start,4) * 1000}ms and class was {pred}\")\n",
    "                            \n",
    "tokenized_seq = tokenize(sequence) \n",
    "# neuron model\n",
    "print(label)\n",
    "run_benchmark(model_neuron, tokenized_seq)\n",
    "run_benchmark(model, tokenized_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare complete MRPC Test dataset\n",
    "\n",
    "using `batch_size=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,load_metric\n",
    "import tensorflow  # to workaround a protobuf version conflict issue\n",
    "import torch\n",
    "import torch.neuron\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "task = \"mrpc\"\n",
    "split=\"validation\"\n",
    "all_datasets = load_dataset(\"glue\", task)\n",
    "metric = load_metric(\"glue\", task)\n",
    "dataset= all_datasets[split]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "max_length=128\n",
    "padding='max_length'\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    texts = (examples['sentence1'], examples['sentence2'])\n",
    "    result = tokenizer(*texts, padding=padding, max_length=max_length, truncation=True,return_tensors=\"pt\")\n",
    "    result[\"labels\"] = examples[\"label\"]\n",
    "    return result\n",
    "\n",
    "\n",
    "def do_test(raw_dataset,model,model_type):\n",
    "    processed_dataset = raw_dataset.map(preprocess_function)\n",
    "    processed_dataset = processed_dataset.select(range(1000))\n",
    "    model_start = time.perf_counter()\n",
    "#     model_type = 'neuron' if isinstance(model, torch.jit.ScriptModule) else 'torch'\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(processed_dataset):\n",
    "            input_ids = torch.tensor(batch['input_ids'])\n",
    "            attention_mask = torch.tensor(batch['attention_mask'])\n",
    "            token_type_ids = torch.tensor(batch['token_type_ids'])\n",
    "            outputs = model(*[input_ids,attention_mask,token_type_ids])\n",
    "            predictions = outputs[0][0].argmax().item()\n",
    "            metric.add_batch(predictions=[predictions],references=[batch[\"labels\"]])\n",
    "        \n",
    "    eval_metric = metric.compute()\n",
    "    model_stop = time.perf_counter()\n",
    "    total_time = round(model_stop - model_start,4)*1000\n",
    "    average_time =  round(total_time/len(processed_dataset),4)\n",
    "    return {'model_type':model_type,**eval_metric,'total_time':f\"{total_time}ms\",'average_time':f\"{average_time}ms\"}   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-54675240ca23d76a.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-54675240ca23d76a.arrow\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py:727: UserWarning: Neuron runtime cannot be initialized; falling back to CPU execution\n",
      "Tensor output are ** NOT CALCULATED ** during CPU execution and only indicate tensor shape (Triggered internally at  /opt/workspace/KaenaPyTorchRuntime/neuron_op/neuron_op_impl.cpp:38.)\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_type': 'pytorch', 'accuracy': 0.852, 'f1': 0.8987688098495212, 'total_time': '246881.30000000002ms', 'average_time': '246.8813ms'}\n",
      "{'model_type': 'neuron', 'accuracy': 0.685, 'f1': 0.8130563798219584, 'total_time': '2283.7999999999997ms', 'average_time': '2.2838ms'}\n"
     ]
    }
   ],
   "source": [
    "model_neuron = torch.jit.load('bert_neuron.pt')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\", return_dict=False)\n",
    "\n",
    "\n",
    "\n",
    "model_res=do_test(dataset, model,'pytorch')\n",
    "model_neuron_res = do_test(dataset, model_neuron,'neuron')\n",
    "\n",
    "print(model_res)\n",
    "print(model_neuron_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the model into an non AWS optimized TorchScript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py:195: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/modeling_utils.py:1790: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import transformers\n",
    "\n",
    "# Build tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\", return_dict=False)\n",
    "\n",
    "\n",
    "sequence = [\"The company HuggingFace is based in New York City\",\n",
    "            \"HuggingFace's headquarters are situated in Manhattan\"]\n",
    "\n",
    "\n",
    "max_length=128\n",
    "paraphrase = tokenizer.encode_plus(sequence, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "example_inputs_paraphrase = paraphrase['input_ids'], paraphrase['attention_mask'], paraphrase['token_type_ids']\n",
    "\n",
    "# Run torch.jit.trace to generate a TorchScript \n",
    "model_torch = torch.jit.trace(model, example_inputs_paraphrase)\n",
    "\n",
    "# Verify the TorchScript works on both example inputs\n",
    "paraphrase_classification_logits_neuron = model_neuron(*example_inputs_paraphrase)\n",
    "# Save the TorchScript for later use\n",
    "model_torch.save('bert_torch.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4/cache-54675240ca23d76a.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_type': 'torchscript',\n",
       " 'accuracy': 0.852,\n",
       " 'f1': 0.8987688098495212,\n",
       " 'total_time': '277930.2ms',\n",
       " 'average_time': '277.9302ms'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_torch = torch.jit.load('bert_torch.pt')\n",
    "\n",
    "\n",
    "model_torch_res = do_test(dataset, model_torch,'torchscript')\n",
    "model_torch_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([model_res,model_neuron_res,model_torch_res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>total_time</th>\n",
       "      <th>average_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.898769</td>\n",
       "      <td>246881.30000000002ms</td>\n",
       "      <td>246.8813ms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neuron</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.813056</td>\n",
       "      <td>2283.7999999999997ms</td>\n",
       "      <td>2.2838ms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>torchscript</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.898769</td>\n",
       "      <td>277930.2ms</td>\n",
       "      <td>277.9302ms</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model_type  accuracy        f1            total_time average_time\n",
       "0      pytorch     0.852  0.898769  246881.30000000002ms   246.8813ms\n",
       "1       neuron     0.685  0.813056  2283.7999999999997ms     2.2838ms\n",
       "2  torchscript     0.852  0.898769            277930.2ms   277.9302ms"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
