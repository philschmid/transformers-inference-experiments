{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e15aa61-1e2e-4abd-9563-22f1e1cf1bf2",
   "metadata": {},
   "source": [
    "# Exporting ü§ó transformers model to ONNX\n",
    "\n",
    "\n",
    "Under the hood the process is sensibly the following:\n",
    "\n",
    "- Allocate the model from transformers (PyTorch or TensorFlow)\n",
    "- Forward dummy inputs through the model this way ONNX can record the set of operations executed\n",
    "- Optionally define dynamic axes on input and output tensors\n",
    "- Save the graph along with the network parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41c521e-0178-4c76-97e6-ce3e38f67256",
   "metadata": {},
   "source": [
    "## Default compiling (not task specific)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84fea93-52be-44a1-bf3c-569c5264da16",
   "metadata": {},
   "source": [
    "[ONNX x Pytorch Opset Version Table](https://github.com/onnx/onnx/blob/master/docs/Versioning.md#released-versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47156d92-3dd9-4ec4-8690-4d051ff59d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX opset version set to: 13\n",
      "Loading pipeline (model: bert-base-cased-finetuned-mrpc, tokenizer: bert-base-cased-finetuned-mrpc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50c1b254044417b8b942d3dfe6b9bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433297515.0, style=ProgressStyle(descri‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating folder models\n",
      "Using framework PyTorch: 1.8.1+cu102\n",
      "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input token_type_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_0 with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_1 with shape: {0: 'batch'}\n",
      "Ensuring inputs are in correct order\n",
      "position_ids is not present in the generated input list.\n",
      "Generated inputs order: ['input_ids', 'attention_mask', 'token_type_ids']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/modeling_utils.py:1790: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors\n"
     ]
    }
   ],
   "source": [
    "!rm -rf models/\n",
    "from pathlib import Path\n",
    "from transformers.convert_graph_to_onnx import convert\n",
    "\n",
    "# Handles all the above steps for you\n",
    "convert(framework=\"pt\", # The framework the pipeline is backed by (\"pt\" or \"tf\")\n",
    "        model=\"bert-base-cased-finetuned-mrpc\", #  The name of the model to load for the pipeline\n",
    "        output=Path(\"models/bert.onnx\"), # The path where the ONNX graph will be stored\n",
    "        opset=11 # version of the ONNX operator set to use \n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7cc86-1fee-4e38-b93f-5128c4207202",
   "metadata": {},
   "source": [
    "## Mixed Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc6f4a-fe9c-469b-9922-cb1891b7ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or you are working with Tensorflow(tf.keras) models or pytorch models other than bert\n",
    "\n",
    "# !pip install onnxruntime-tools\n",
    "from onnxruntime_tools import optimizer\n",
    "\n",
    "# Mixed precision conversion for bert-base-cased model converted from Pytorch\n",
    "optimized_model = optimizer.optimize_model(\"bert-base-cased.onnx\", model_type='bert', num_heads=12, hidden_size=768)\n",
    "optimized_model.convert_model_float32_to_float16()\n",
    "optimized_model.save_model_to_file(\"bert-base-cased.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9487ce-97f8-4059-b460-813eb7227a41",
   "metadata": {},
   "source": [
    "# Inference Session: optimize compiled model\n",
    "\n",
    "Inference is done using a specific backend definition which turns on hardware specific optimizations of the graph.\n",
    "\n",
    "Optimizations are basically of three kinds:\n",
    "\n",
    "- Constant Folding: Convert static variables to constants in the graph\n",
    "- Deadcode Elimination: Remove nodes never accessed in the graph\n",
    "- Operator Fusing: Merge multiple instruction into one (Linear -> ReLU can be fused to be LinearReLU)\n",
    "\n",
    "ONNX Runtime automatically applies most optimizations by setting specific SessionOptions.\n",
    "\n",
    "_Note: Some of the latest optimizations that are not yet integrated into ONNX Runtime are available in optimization script that tunes models for the best performance._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236079ce-61de-49a5-bcd8-f9faf4fc3c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.transformers import optimizer\n",
    "from onnxruntime.transformers.onnx_model_bert import BertOptimizationOptions\n",
    "\n",
    "# disable embedding layer norm optimization for better model size reduction\n",
    "opt_options = BertOptimizationOptions('bert')\n",
    "opt_options.enable_embed_layer_norm = False\n",
    "\n",
    "# opimtimize compiled model\n",
    "opt_model = optimizer.optimize_model(\n",
    "    'models/bert.onnx',\n",
    "    'bert', \n",
    "    num_heads=12,\n",
    "    hidden_size=768,\n",
    "    optimization_options=opt_options)\n",
    "opt_model.('bert.opt.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae966fc3-f2f1-4a20-aeec-e34fb4abc056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "from psutil import cpu_count\n",
    "\n",
    "# Constants from the performance optimization available in onnxruntime\n",
    "# It needs to be done before importing onnxruntime\n",
    "environ[\"OMP_NUM_THREADS\"] = str(cpu_count(logical=True))\n",
    "environ[\"OMP_WAIT_POLICY\"] = 'ACTIVE'\n",
    "\n",
    "from onnxruntime import GraphOptimizationLevel, InferenceSession, SessionOptions, get_all_providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abd3d353-3392-4aff-8b93-0856c3e45710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from time import time\n",
    "\n",
    "def create_model_for_provider(model_path: str, provider: str) -> InferenceSession: \n",
    "    assert provider in get_all_providers(), f\"provider {provider} not found, {get_all_providers()}\"\n",
    "\n",
    "    # Few properties that might have an impact on performances (provided by MS)\n",
    "    options = SessionOptions()\n",
    "    options.intra_op_num_threads = 1\n",
    "    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "    # Load the model as a graph and prepare the CPU backend \n",
    "    session = InferenceSession(model_path, options, providers=[provider])\n",
    "    session.disable_fallback()\n",
    "\n",
    "    return session\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def track_infer_time(buffer: [int]):\n",
    "    start = time()\n",
    "    yield\n",
    "    end = time()\n",
    "\n",
    "    buffer.append(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9abf64-b5ba-4a3a-bf2b-ec61b56e91d3",
   "metadata": {},
   "source": [
    "## using our optimized ONNX model running on CPU\n",
    "\n",
    "When the model is loaded for inference over a specific provider, for instance CPUExecutionProvider as above, an optimized graph can be saved. This graph will might include various optimizations, and you might be able to see some higher-level operations in the graph (through Netron for instance) such as:\n",
    "\n",
    "- EmbedLayerNormalization\n",
    "- Attention\n",
    "- FastGeLU\n",
    "\n",
    "These operations are an example of the kind of optimization onnxruntime is doing, for instance here gathering multiple operations into bigger one (Operator Fusing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "305dadb9-55a1-4cb4-ab03-6eff758264eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "buffer=[]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "cpu_model = create_model_for_provider(\"models/bert.onnx\", \"CPUExecutionProvider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b143706-c915-4218-a048-2ffd1810e61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris is the ≈Å of France.\n",
      "Paris is the ‡§Ø of France.\n",
      "Paris is the –Ø of France.\n",
      "Paris is the ‡Ωë of France.\n",
      "Paris is the ƒè of France.\n",
      "Paris is the –Æ of France.\n",
      "Paris is the [unused54] of France.\n",
      "Paris is the ·µè of France.\n",
      "Paris is the ÿ¶ of France.\n",
      "Paris is the ¬® of France.\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "text = \"Paris is the \" + tokenizer.mask_token + \" of France.\"\n",
    "input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
    "inputs_onnx = {k: v.cpu().detach().numpy() for k, v in input.items()}\n",
    "\n",
    "mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
    "\n",
    "\n",
    "# Run the model (None = get all the outputs)\n",
    "output = cpu_model.run(None, inputs_onnx)\n",
    "\n",
    "logits = torch.from_numpy(output[0])\n",
    "softmax = F.softmax(logits, dim = -1)\n",
    "mask_word = softmax[0, mask_index, :]\n",
    "top_10 = torch.topk(mask_word, 10, dim = 1)[1][0]\n",
    "for token in top_10:\n",
    "    word = tokenizer.decode([token])\n",
    "    new_sentence = text.replace(tokenizer.mask_token, word)\n",
    "    print(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beb301ee-1161-427b-94d7-b52d0766de04",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cpu_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d489808b1c49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minputs_onnx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcpu_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_onnx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtoken_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cpu_model' is not defined"
     ]
    }
   ],
   "source": [
    "# numpy version\n",
    "\n",
    "input = tokenizer(text, return_tensors=\"np\")\n",
    "inputs_onnx = {k: v for k, v in input.__dict__[\"data\"].items()}\n",
    "\n",
    "output = cpu_model.run(None, inputs_onnx)\n",
    "\n",
    "token_logits = output[0]\n",
    "\n",
    "mask_token_index = np.where(input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits_onnx1 = token_logits[0, mask_token_index, :]\n",
    "\n",
    "score = np.exp(mask_token_logits_onnx1) / np.exp(mask_token_logits_onnx1).sum(-1, keepdims=True)\n",
    "\n",
    "top_5_idx = (-score[0]).argsort()[:5]\n",
    "top_5_values = score[0][top_5_idx]\n",
    "\n",
    "result = []\n",
    "\n",
    "for token, s in zip(top_5_idx.tolist(), top_5_values.tolist()):\n",
    "    result.append(f\"{text.replace(tokenizer.mask_token, tokenizer.decode([token]))} (score: {s})\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77947b58-ed1a-4f3b-9dfb-bde53e4f82e5",
   "metadata": {},
   "source": [
    "# Wrapper for Pipeline\n",
    "\n",
    "over write `model.forward()` with onnx specifica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "59b05607-0f50-41a5-b966-96cf2846d2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Pipeline, TextClassificationPipeline \n",
    "from transformers.tokenization_utils import TruncationStrategy\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM\n",
    "# Download configuration from huggingface.co and cache.\n",
    "config = AutoConfig.from_pretrained('bert-base-cased-finetuned-mrpc')\n",
    "model = AutoModelForMaskedLM.from_config(config)\n",
    "\n",
    "class OnnxTextClassificationPipeline(FillMaskPipeline):\n",
    "  #todo create a batched version of text-classification\n",
    "  # can we overtake a nested nested function \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__( **kwargs)\n",
    "\n",
    "#     def _parse_and_tokenize(\n",
    "#       self, inputs, padding=True, add_special_tokens=True, truncation=TruncationStrategy.DO_NOT_TRUNCATE, **kwargs\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Parse arguments and tokenize\n",
    "#         \"\"\"\n",
    "#         # Parse arguments\n",
    "#         processed_inputs = []\n",
    "\n",
    "#         inputs = inputs if isinstance(inputs,list) else [inputs]\n",
    "#         for input in inputs:\n",
    "#         tok = self.tokenizer(\n",
    "#             input,\n",
    "#             add_special_tokens=add_special_tokens,\n",
    "#             padding=padding,\n",
    "#             truncation=truncation,\n",
    "#         )\n",
    "#         processed_inputs.append(tok)\n",
    "\n",
    "#         return processed_inputs\n",
    "\n",
    "    def _forward(self, inputs, return_tensors=False):\n",
    "        inputs_onnx = {k: v for k, v in input.__dict__[\"data\"].items()}\n",
    "        outputs = cpu_model.run(None, inputs)\n",
    "\n",
    "        real_output = normalize_onnx_outputs(outputs, output_names)\n",
    "        print(\"Successful inference on ONNX\")\n",
    "        return real_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e95a14dd-d061-4fb6-bf2f-185df5275c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = OnnxTextClassificationPipeline(model=model,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d7aabca4-990b-479c-a07c-f91ded92d7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful inference on ONNX\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-46ac3e5f78d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Paris is the \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" of France.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/pipelines/fill_mask.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, targets, top_k, *args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tf\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "text = \"Paris is the \" + tokenizer.mask_token + \" of France.\"\n",
    "x(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "14b40298-4c64-4a2f-bc7a-23c5810c67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_names = [input_.name for input_ in cpu_model.get_inputs()]\n",
    "output_names = [output_.name for output_ in cpu_model.get_outputs()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7c0a9f8a-0b46-460d-9a3d-aed1d45b01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Paris is the \" + tokenizer.mask_token + \" of France.\"\n",
    "input = tokenizer(text, return_tensors=\"np\")\n",
    "inputs_onnx = {k: v for k, v in input.__dict__[\"data\"].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5ffe8832-8ace-49b8-b3b6-4be55faeab70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful inference on ONNX\n"
     ]
    }
   ],
   "source": [
    "res = onnx_forward(inputs_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "40553828-22c9-4f19-9ee6-389f24939129",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = [\"The company HuggingFace is based in New York City\",\n",
    "           # \"Apples are especially bad for your health\"\n",
    "           \"HuggingFace's headquarters are situated in Manhattan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "89b1e784-77f1-4088-b0df-87fc3b1f4ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paris is the ≈Å of France. (score: 0.004327240400016308)',\n",
       " 'Paris is the ‡§Ø of France. (score: 0.0032415653113275766)',\n",
       " 'Paris is the –Ø of France. (score: 0.003234578762203455)',\n",
       " 'Paris is the ‡Ωë of France. (score: 0.0031752123031765223)',\n",
       " 'Paris is the ƒè of France. (score: 0.003119068220257759)']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy version\n",
    "\n",
    "\n",
    "output = cpu_model.run(None, inputs_onnx)\n",
    "\n",
    "token_logits = output[0]\n",
    "\n",
    "mask_token_index = np.where(input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits_onnx1 = token_logits[0, mask_token_index, :]\n",
    "\n",
    "score = np.exp(mask_token_logits_onnx1) / np.exp(mask_token_logits_onnx1).sum(-1, keepdims=True)\n",
    "\n",
    "top_5_idx = (-score[0]).argsort()[:5]\n",
    "top_5_values = score[0][top_5_idx]\n",
    "\n",
    "result = []\n",
    "\n",
    "for token, s in zip(top_5_idx.tolist(), top_5_values.tolist()):\n",
    "    result.append(f\"{text.replace(tokenizer.mask_token, tokenizer.decode([token]))} (score: {s})\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f0b72a96-2613-42cd-83b5-b876f96098e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9, 768)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_model.run(None, inputs_onnx)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2638412-c0a9-49cd-b761-cd4aee913a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased-finetuned-mrpc were not used when initializing BertForMaskedLM: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-cased-finetuned-mrpc and are newly initialized: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SessionOptions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9f2199b2507d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0minference_api_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_onnx_model_on_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"models/bert.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/onnx/inference_api_wrapper.py\u001b[0m in \u001b[0;36mload_onnx_model_on_pipeline\u001b[0;34m(nlp, quantized_output)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_onnx_model_on_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantized_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0msess_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSessionOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# Set graph optimization level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SessionOptions' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM,pipeline\n",
    "import numpy as np\n",
    "# from inference_api_wrapper import load_onnx_model_on_pipeline\n",
    "import inference_api_wrapper\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForMaskedLM.from_pretrained('bert-base-cased-finetuned-mrpc')\n",
    "\n",
    "\n",
    "\n",
    "pipe = pipeline('fill-mask',model=model,tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "inference_api_wrapper.load_onnx_model_on_pipeline(pipe,\"models/bert.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fa25c9d-a6ce-4ddf-8172-bb011fc94c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error running onnx_forward: __init__() keywords must be strings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'Paris is thejid of France.',\n",
       "  'score': 0.004590186290442944,\n",
       "  'token': 26151,\n",
       "  'token_str': '##jid'},\n",
       " {'sequence': 'Paris is theregation of France.',\n",
       "  'score': 0.003801520448178053,\n",
       "  'token': 22998,\n",
       "  'token_str': '##regation'},\n",
       " {'sequence': 'Paris is thecourse of France.',\n",
       "  'score': 0.0032074858900159597,\n",
       "  'token': 16461,\n",
       "  'token_str': '##course'},\n",
       " {'sequence': 'Paris is themark of France.',\n",
       "  'score': 0.0030862404964864254,\n",
       "  'token': 8519,\n",
       "  'token_str': '##mark'},\n",
       " {'sequence': 'Paris is theina of France.',\n",
       "  'score': 0.0027963079046458006,\n",
       "  'token': 2983,\n",
       "  'token_str': '##ina'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Paris is the \" + tokenizer.mask_token + \" of France.\"\n",
    "\n",
    "pipe(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3307cac0-0a92-4dd3-803f-20067cf23231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
