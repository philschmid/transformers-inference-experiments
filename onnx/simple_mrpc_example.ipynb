{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "native-earth",
   "metadata": {},
   "source": [
    "# Exporting & optimiazing ðŸ¤— transformers model to ONNX\n",
    "\n",
    "* [Example from transformers](https://github.com/huggingface/transformers/blob/master/notebooks/04-onnx-export.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90886c78-6bc4-471c-803d-e3ba83d76bb5",
   "metadata": {},
   "source": [
    "Under the hood the process is sensibly the following:\n",
    "\n",
    "converting:\n",
    "1. Allocate the model from transformers (PyTorch or TensorFlow)\n",
    "2. Forward dummy inputs through the model this way ONNX can record the set of operations executed\n",
    "3. Optionally define dynamic axes on input and output tensors\n",
    "4. Save the graph along with the network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a8c4a68-be20-419b-99b3-e47ac148b722",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"bert-base-cased-finetuned-mrpc\"\n",
    "pipeline=\"sentiment-analysis\"\n",
    "save_path=\"onnx\"\n",
    "max_length=128\n",
    "opset_version=11\n",
    "export_model_path=f\"{save_path}/{model_id}.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06f6d65a-3f31-4649-83ed-5641920cfc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "from psutil import cpu_count\n",
    "\n",
    "# Constants from the performance optimization available in onnxruntime\n",
    "# It needs to be done before importing onnxruntime\n",
    "environ[\"OMP_NUM_THREADS\"] = str(cpu_count(logical=True))\n",
    "environ[\"OMP_WAIT_POLICY\"] = 'ACTIVE'\n",
    "\n",
    "from onnxruntime import GraphOptimizationLevel, InferenceSession, SessionOptions, get_all_providers\n",
    "\n",
    "class OnnxModel:\n",
    "    def __init__(self, model_path: str, provider: str):\n",
    "        assert provider in get_all_providers(), f\"provider {provider} not found, {get_all_providers()}\"\n",
    "        self.options = self._set_options()\n",
    "        self.model=InferenceSession(model_path, self.options, providers=[provider])\n",
    "        # Load the model as a graph and prepare the CPU backend \n",
    "        self.model.disable_fallback()\n",
    "\n",
    "    def __call__(self, input):\n",
    "        return self.model.run(None, input)[0]\n",
    "  \n",
    "    def _set_options(self):\n",
    "        # Few properties that might have an impact on performances (provided by MS)\n",
    "        options = SessionOptions()\n",
    "        options.intra_op_num_threads = 1\n",
    "        options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "        return options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182aa13f-ea5a-442d-9f63-fbf0be5adc01",
   "metadata": {},
   "source": [
    "# Converting model to onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfacfa0-56e5-426a-8d41-a9c9d988e765",
   "metadata": {},
   "source": [
    "## automatic conversion, build into `transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbc69c3f-1292-46a1-a1f3-7db16f6672a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX opset version set to: 11\n",
      "Loading pipeline (model: bert-base-cased-finetuned-mrpc, tokenizer: bert-base-cased-finetuned-mrpc)\n",
      "Creating folder onnx\n",
      "Using framework PyTorch: 1.7.1+cpu\n",
      "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input token_type_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_0 with shape: {0: 'batch'}\n",
      "Ensuring inputs are in correct order\n",
      "position_ids is not present in the generated input list.\n",
      "Generated inputs order: ['input_ids', 'attention_mask', 'token_type_ids']\n"
     ]
    }
   ],
   "source": [
    "!rm -rf onnx/\n",
    "from pathlib import Path\n",
    "from transformers.convert_graph_to_onnx import convert\n",
    "\n",
    "# Handles all the above steps for you\n",
    "convert(model=model_id,\n",
    "        output=Path(f\"transformers/{export_model_path}\"),\n",
    "        pipeline_name=pipeline,\n",
    "        opset=opset_version,\n",
    "        framework='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e658d8-5f7e-4476-b54e-be2a6451e122",
   "metadata": {},
   "source": [
    "## manual conversion with `torch.onnx.export`\n",
    "\n",
    "1. create sample input\n",
    "2. `forward()` to get outputs\n",
    "3. create `input_names` and `output_names`\n",
    "3. create `dynamic_axes` -> input/output tensors where the shape might change -> basically every input and output.\n",
    "4. convert model with `torch.onnx.export``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b78d39b2-4d1d-433c-9139-0a5bee0d7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers.file_utils import ModelOutput\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "# Setup some example inputs\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "paraphrase = tokenizer.encode_plus(sequence_0, sequence_2, max_length=max_length, padding='longest', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**paraphrase) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3953f7c9-3d4b-4fa7-a9b8-e550b4b2ab34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created input_names:\n",
      "['input_ids', 'token_type_ids', 'attention_mask']\n",
      "created output_names:\n",
      "['output_0']\n",
      "created dynamic_axes:\n",
      "{'input_ids': {0: 'batch_size', 1: 'sequence'}, 'token_type_ids': {0: 'batch_size', 1: 'sequence'}, 'attention_mask': {0: 'batch_size', 1: 'sequence'}, 'output_0': {0: 'batch_size'}}\n"
     ]
    }
   ],
   "source": [
    "# generate input and output names/keys\n",
    "input_names = list(paraphrase.keys())\n",
    "output_names = [f\"output_{i}\" for i in range(len(outputs))]\n",
    "\n",
    "# Generate dynamic axes, with batching -> inputs/outputs with potential dynamic shape\n",
    "symbolic_names = {0: 'batch_size', 1: 'sequence'} #TODO: Exaplain\n",
    "\n",
    "input_dynamic_axes = {input_key: symbolic_names for input_key in input_names}\n",
    "output_dynamic_axes = {output_key: {0: 'batch_size'} for output_key in output_names}\n",
    "dynamic_axes = dict(input_dynamic_axes, **output_dynamic_axes)\n",
    "\n",
    "print(f\"created input_names:\")\n",
    "print(input_names)\n",
    "print(f\"created output_names:\")\n",
    "print(output_names)\n",
    "print(f\"created dynamic_axes:\")\n",
    "print(dynamic_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7afc744b-dce6-4048-8b61-6eb3f01d2ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported at  onnx/bert-base-cased-finetuned-mrpc.onnx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.makedirs(export_model_path.replace('.onnx',''),exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(model,                               # model being run\n",
    "                      args=tuple(paraphrase.values()),     # model input (or a tuple for multiple inputs)\n",
    "                      f=export_model_path,                 # where to save the model (can be a file or file-like object)\n",
    "                      opset_version=opset_version,         # the ONNX version to export the model to\n",
    "                      do_constant_folding=True,            # whether to execute constant folding for optimization\n",
    "                      enable_onnx_checker=True, \n",
    "                      use_external_data_format=False,\n",
    "                      input_names=input_names,             # the model's input names  'input_ids', 'token_type_ids', 'attention_mask'\n",
    "                      output_names=output_names,           # the model's output names 'output_0'\n",
    "                      dynamic_axes=dynamic_axes)           # inputs/outputs with potential dynamic shape -> mostly all\n",
    "\n",
    "print(\"Model exported at \", export_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c13556-8220-4643-8f6e-9d9bd08347d7",
   "metadata": {},
   "source": [
    "# Optimize exported model\n",
    "\n",
    "Optimizations are basically of three kinds:\n",
    "\n",
    "1. Constant Folding: Convert static variables to constants in the graph\n",
    "2. Deadcode Elimination: Remove nodes never accessed in the graph\n",
    "3. Operator Fusing: Merge multiple instruction into one (Linear -> ReLU can be fused to be LinearReLU)\n",
    "\n",
    "\n",
    "Optimizer script managed bei onnx: https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/optimizer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eb9c6f-f475-4297-a7d5-b1050b7a1bfe",
   "metadata": {},
   "source": [
    "## Optimizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc1bfb6-5e7e-45e6-ae4f-de776cf9ab35",
   "metadata": {},
   "source": [
    "the optimize method we have in `transformers` only rely on online optimizations when loading the ONNX graph. Any offline transformations, like `SkipLayerNorm`, `EmbedLayerNorm`, `Attention`, `FastGeLU` might not be applied from the online version\n",
    "\n",
    "Offline also allows to convert fp32 models to AMP on GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b80701-3a29-4a31-af5b-5d1c1f5d6427",
   "metadata": {},
   "source": [
    "### manual offline optimization with `optimizer.optimize_model` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f2819-7039-41bd-ab4c-a92d1dec087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all optimization layers\n",
    "# optimization_options.enable_gelu = False\n",
    "# optimization_options.enable_layer_norm = False\n",
    "# optimization_options.enable_attention = False\n",
    "# optimization_options.enable_skip_layer_norm = False\n",
    "# optimization_options.enable_embed_layer_norm = False\n",
    "# optimization_options.enable_bias_skip_layer_norm = False\n",
    "# optimization_options.enable_bias_gelu = False\n",
    "# optimization_options.enable_gelu_approximation = True\n",
    "# optimization_options.use_raw_attention_mask(False)\n",
    "# optimization_options.disable_attention_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7302d44c-8724-48b2-a523-49360a4eabb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize transformer-based models with onnxruntime-tools\n",
    "from onnxruntime.transformers import optimizer\n",
    "from onnxruntime.transformers.onnx_model_bert import BertOptimizationOptions\n",
    "\n",
    "# disable embedding layer norm optimization for better model size reduction\n",
    "optimized_options = BertOptimizationOptions('bert')\n",
    "optimized_options.enable_embed_layer_norm = False\n",
    "\n",
    "optimized_model = optimizer.optimize_model(\n",
    "    export_model_path,\n",
    "    'bert', \n",
    "    num_heads=12,\n",
    "    hidden_size=768,\n",
    "    opt_level=99,\n",
    "    optimization_options=optimized_options)\n",
    "\n",
    "optimized_model.save_model_to_file(f\"{save_path}/optimized_{model_id}.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "237c6a3f-9f5d-4b86-aea5-20f66976f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model = optimizer.optimize_model(export_model_path, opt_level=1, use_gpu=False, only_onnxruntime=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61a90b42-5ab1-4a3f-a5de-1352f68380c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 216.647809\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('Size (MB):', os.path.getsize(f\"{save_path}/optimized_{model_id}.onnx\")/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578f7fc3-9ada-4a7b-a9fd-46ab83fe832c",
   "metadata": {},
   "source": [
    "### automatic opmtimization, build into `transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "668c32a6-cecc-416f-814d-6d1fdc10cb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized model has been written at transformers/onnx/bert-base-cased-finetuned-mrpc-optimized.onnx: âœ”\n",
      "/!\\ Optimized model contains hardware specific operators which might not be portable. /!\\\n",
      "Size (MB): 433.319523\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers.convert_graph_to_onnx import optimize\n",
    "\n",
    "optimize(Path(f\"transformers/{export_model_path}\"))\n",
    "\n",
    "import os\n",
    "print('Size (MB):', os.path.getsize(f\"transformers/{export_model_path}\")/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c30fe-547a-4d18-bff3-5c3255ec3af8",
   "metadata": {},
   "source": [
    "# Quantize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6379f84f-2a59-4588-85a7-615f1cf3ab12",
   "metadata": {},
   "source": [
    "### manual conversion with `quantize_helper.quantize_onnx_model` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ffc89c87-5a16-444d-9e68-99fc11e6b664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:onnxruntime.quantization.quantize is deprecated.\n",
      "         Please use quantize_static for static quantization, quantize_dynamic for dynamic quantization.\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.transformers.quantize_helper import QuantizeHelper\n",
    "\n",
    "onnx_model_path = f\"transformers/onnx/bert-base-cased-finetuned-mrpc-optimized.onnx\"\n",
    "quantized_model_path = f\"transformers/onnx/bert-base-cased-finetuned-mrpc-optimized-quantized.onnx\"\n",
    "\n",
    "QuantizeHelper.quantize_onnx_model(onnx_model_path, quantized_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c37a4f-924e-402d-b424-6a6d5f036537",
   "metadata": {},
   "source": [
    "### automatic opmtimization, build into `transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb97d79-b6f4-424a-bb27-460619495713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from transformers.convert_graph_to_onnx import quantize\n",
    "\n",
    "quantize(Path(f\"transformers/{export_model_path}\"))\n",
    "\n",
    "import os\n",
    "print('Size (MB):', os.path.getsize(f\"transformers/{export_model_path}\")/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dca61b-bde5-471f-9a7e-472b5ac44f03",
   "metadata": {},
   "source": [
    "## Test inference with exported model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "22823589-e0b2-47e5-abf3-c604cb74555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "provider = \"CPUExecutionProvider\"\n",
    "#onnx_model = OnnxModel(f\"{save_path}/optimized_{model_id}.onnx\", provider)\n",
    "onnx_model = OnnxModel(f\"transformers/onnx/bert-base-cased-finetuned-mrpc-optimized.onnx\", provider)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Setup some example inputs\n",
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
    "\n",
    "paraphrase = tokenizer.encode_plus(sequence_0, sequence_2, max_length=max_length, padding='longest', truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf297d4-7be4-4f21-b3e6-672adff6a6ed",
   "metadata": {},
   "source": [
    "#### onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63c2f1db-2e20-4fa6-9b15-9fb2bdab14a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_onnx = {k: v.cpu().detach().numpy() for k, v in paraphrase.items()}\n",
    "\n",
    "outputs = onnx_model(inputs_onnx)\n",
    "outputs[0].argmax().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f479a63a-2b1a-425a-9491-f70fcf7b7d01",
   "metadata": {},
   "source": [
    "#### pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c865cc2e-358d-41d3-8082-9f396c359b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "\n",
    "outputs = model(**paraphrase) \n",
    "outputs[0][0].argmax().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d27b5e-cf2c-4f54-80ee-3fdaf2efc529",
   "metadata": {},
   "source": [
    "#### model size comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d1870b9-4508-4d50-ba14-1fe23d0e200a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 433.319523\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('Size (MB):', os.path.getsize(export_model_path)/1e6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfd765f-c621-453a-9c74-521b1de1e91b",
   "metadata": {},
   "source": [
    "### benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee14b7c0-3886-41ec-8871-6d4f2fd61b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset,load_metric\n",
    "import time\n",
    "import torch\n",
    "\n",
    "task = \"mrpc\"\n",
    "split=\"validation\"\n",
    "all_datasets = load_dataset(\"glue\", task)\n",
    "metric = load_metric(\"glue\", task)\n",
    "dataset= all_datasets[split]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "max_length=128\n",
    "padding='longest'\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    texts = (examples['sentence1'], examples['sentence2'])\n",
    "    result = tokenizer(*texts, padding=padding, max_length=max_length, truncation=True,return_tensors=\"pt\")\n",
    "    result[\"labels\"] = examples[\"label\"]\n",
    "    return result\n",
    "\n",
    "\n",
    "def do_test(name='',raw_dataset=None,model=None,model_type='',samples=None):\n",
    "    processed_dataset = raw_dataset.map(preprocess_function)\n",
    "    processed_dataset = processed_dataset.select(range(samples))\n",
    "    model_start = time.perf_counter()\n",
    "    if model_type == 'onnx':\n",
    "        for step, batch in enumerate(processed_dataset):\n",
    "            destructed_dict = {'input_ids': torch.tensor(batch['input_ids']),\n",
    "                               'attention_mask': torch.tensor(batch['attention_mask']),\n",
    "                               'token_type_ids': torch.tensor(batch['token_type_ids'])\n",
    "                              }\n",
    "            inputs_onnx = {k: v.cpu().detach().numpy() for k, v in destructed_dict.items()}\n",
    "            outputs = model(inputs_onnx)\n",
    "            predictions = outputs[0].argmax().item()\n",
    "            metric.add_batch(predictions=[predictions],references=[batch[\"labels\"]])\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            for step, batch in enumerate(processed_dataset):\n",
    "                input_ids = torch.tensor(batch['input_ids'])\n",
    "                attention_mask = torch.tensor(batch['attention_mask'])\n",
    "                token_type_ids = torch.tensor(batch['token_type_ids'])\n",
    "                outputs = model(*[input_ids,attention_mask,token_type_ids])\n",
    "                predictions = outputs[0][0].argmax().item()\n",
    "                metric.add_batch(predictions=[predictions],references=[batch[\"labels\"]])\n",
    "        \n",
    "    eval_metric = metric.compute()\n",
    "    model_stop = time.perf_counter()\n",
    "    total_time = round(model_stop - model_start,4)*1000\n",
    "    average_time =  round(total_time/len(processed_dataset),4)\n",
    "    return {'name':name,\n",
    "            'model_type':model_type,\n",
    "            **eval_metric,\n",
    "            'total_time':f\"{total_time}ms\",\n",
    "            'average_time':f\"{average_time}ms\",\n",
    "            'max_length':max_length,\n",
    "            'samples': len(processed_dataset),\n",
    "            'task': task\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfc92e15-7a07-461b-b450-0d910232a70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d1475cb55d41edafe0dd2d2f977775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=408.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d503fda12b94f809d744f37eaf05ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=408.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "samples=100\n",
    "model_res=do_test(name='pytorch',\n",
    "               raw_dataset=dataset,\n",
    "               model=model,\n",
    "               model_type='pytorch',\n",
    "               samples=samples)\n",
    "    \n",
    "               \n",
    "model_onnx_res=do_test(name='onnx',\n",
    "               raw_dataset=dataset,\n",
    "               model=onnx_model,\n",
    "               model_type='onnx',\n",
    "               samples=samples)\n",
    "\n",
    "model_onnx_optimized = do_test(name='onnx_optimized',\n",
    "                               raw_dataset=dataset,\n",
    "                               model=optimized_model,\n",
    "                               model_type='onnx',\n",
    "                               samples=samples)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([model_res,model_onnx_res])\n",
    "df.to_csv('first-test.csv')\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32f7db0-9193-4191-8271-57d2df59fa1c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Optimize exported model\n",
    "\n",
    "Optimizations are basically of three kinds:\n",
    "\n",
    "1. Constant Folding: Convert static variables to constants in the graph\n",
    "2. Deadcode Elimination: Remove nodes never accessed in the graph\n",
    "3. Operator Fusing: Merge multiple instruction into one (Linear -> ReLU can be fused to be LinearReLU)\n",
    "\n",
    "\n",
    "Optimizer script managed bei onnx: https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/optimizer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d7bd0c-2810-41b1-b391-40a8c6cb5312",
   "metadata": {},
   "source": [
    "# Scripting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68333294-aebf-4a57-bccf-231cddbf2bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== Converting model to ONNX ======\n",
      "ONNX opset version set to: 11\n",
      "Loading pipeline (model: bert-base-cased-finetuned-mrpc, tokenizer: bert-base-cased-finetuned-mrpc)\n",
      "Using framework PyTorch: 1.7.1+cpu\n",
      "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input token_type_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_0 with shape: {0: 'batch'}\n",
      "Ensuring inputs are in correct order\n",
      "position_ids is not present in the generated input list.\n",
      "Generated inputs order: ['input_ids', 'attention_mask', 'token_type_ids']\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py:195: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/modeling_utils.py:1790: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors\n",
      "\n",
      "====== Optimizing ONNX model ======\n",
      "2021-05-09 19:45:30.883663800 [W:onnxruntime:, inference_session.cc:1256 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED. The generated model may contain hardware and execution provider specific optimizations, and should only be used in the same environment the model was optimized for.\n",
      "Optimized model has been written at /home/onnx/models/cli-optimized: âœ”\n",
      "/!\\ Optimized model contains hardware specific operators which might not be portable. /!\\\n",
      "As of onnxruntime 1.4.0, models larger than 2GB will fail to quantize due to protobuf constraint.\n",
      "This limitation will be removed in the next release of onnxruntime.\n",
      "WARNING:root:onnxruntime.quantization.quantize is deprecated.\n",
      "         Please use quantize_static for static quantization, quantize_dynamic for dynamic quantization.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Attention. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator Gelu. No schema registered for this operator.\n",
      "Warning: Unsupported operator LayerNormalization. No schema registered for this operator.\n",
      "Warning: Unsupported operator FusedGemm. No schema registered for this operator.\n",
      "Quantized model has been written at /home/onnx/models/cli-optimized-quantized: âœ”\n",
      "\n",
      "====== Check exported ONNX model(s) ======\n",
      "Checking ONNX model loading from: /home/onnx/models/cli ...\n",
      "Model /home/onnx/models/cli correctly loaded: âœ”\n",
      "Checking ONNX model loading from: /home/onnx/models/cli-optimized ...\n",
      "Model /home/onnx/models/cli-optimized correctly loaded: âœ”\n",
      "Checking ONNX model loading from: /home/onnx/models/cli-optimized-quantized ...\n",
      "Model /home/onnx/models/cli-optimized-quantized correctly loaded: âœ”\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!rm -rf models/ && mkdir models\n",
    "\n",
    "!cd models && python -m transformers.convert_graph_to_onnx \\\n",
    "        --pipeline {pipeline} \\\n",
    "        --model {model_id} \\\n",
    "        --framework 'pt' \\\n",
    "        --opset 11 \\\n",
    "        --check-loading \\\n",
    "        --quantize \\\n",
    "        \"cli\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49c679d7-e687-4c44-ad1b-e6cb72ae1edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli_export_model_path='models/cli-optimized-quantized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b76331a3-ea5e-4274-9bc0-b4f8c7b81431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 110.503015\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('Size (MB):', os.path.getsize(f\"{cli_export_model_path}\")/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99071112-3c7a-4b9e-bfda-9ffa9cc692d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179ca1e1ed3f4acbad136f39e77dedf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=408.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>model_type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>total_time</th>\n",
       "      <th>average_time</th>\n",
       "      <th>max_length</th>\n",
       "      <th>samples</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>onnx_optimized_cli_transformers</td>\n",
       "      <td>onnx</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.894198</td>\n",
       "      <td>95909.7ms</td>\n",
       "      <td>95.9097ms</td>\n",
       "      <td>128</td>\n",
       "      <td>1000</td>\n",
       "      <td>mrpc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              name model_type  accuracy        f1 total_time  \\\n",
       "0  onnx_optimized_cli_transformers       onnx     0.845  0.894198  95909.7ms   \n",
       "\n",
       "  average_time  max_length  samples  task  \n",
       "0    95.9097ms         128     1000  mrpc  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples=1000\n",
    "provider = \"CPUExecutionProvider\"\n",
    "optimized_model = OnnxModel(cli_export_model_path, provider)\n",
    "\n",
    "model_cli_onnx_optimized = do_test(name='onnx_optimized_cli_transformers',\n",
    "                               raw_dataset=dataset,\n",
    "                               model=optimized_model,\n",
    "                               model_type='onnx',\n",
    "                               samples=samples)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([model_cli_onnx_optimized])\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}